# Automating Literature Reviews using Deep Learning Methods

Literature reviews play a crucial role in academic research, providing a comprehensive overview of existing knowledge on a particular topic. They are often times carried out at universities and pharmaceutical companiesand require an extensive amount of reading and analysis of scientific publications.

A typical way of conducting a manual literature review consists of several steps.
First, a huge database, such as PubMed, is queried for thousands of studies based on some keywords and basic criteria
that are determined beforehand and describe the given topic.
Next, all titles and abstracts of the returned studies are manually read and a small percentage (3-5 %) is selected
for further review. This is guided by more precise criteria that are too complicated for data base querying.
Finally, a full text review is performed on the remaining studies in order to make sure that only the relevant ones are
included.

However, given the time-consuming nature of reading thousands of abstracts and hundreds of full studies, there is growing interest in automating certain aspects of literature reviews.
Besides saving time, other benefits of automation include a potentially improved accuracy and objectivity, as human error can be minimized and a difference in approaches by different people is avoided. Additionally, automation tools can efficiently search across various databases and sources, ensuring a more comprehensive coverage of relevant studies.

In this blog, I will look at the tool Litverse and its approach to partially automating literature reviews with the help of GPT-4 and delve into the inner workings of language models like GPT.

## Litverse
Litverse is a tool that aims at automating literature reviews using several NLP and deep learning methods. Its basic concept is to take preselected papers and a number of criteria and keywords as input and to then output a score and ranking for each paper. The scoring process is split up into two steps - a keyword check and a criteria check.

#### Keyword check
To begin, more specific keywords are manually defined and assigned a priority score ranging from 1 to 3, with 1 being the lowest priority and 3 the highest. This approach ensures that essential concepts required in the literature review are given appropriate weightage.

The next step involves scoring the keywords within each paper. Occurrences of the keywords in the paper's title, abstract, and keyword list are counted and multiplied by their respective priority. This results in a keyword score for each paper that serves as a baseline score for the upcoming clustering step.

To facilitate paper clustering, tf-idf (Term Frequency-Inverse Document Frequency) vectorization is used on the abstracts of all papers. Tf-idf is a popular technique in natural language processing that evaluates the importance of a word in a document relative to its occurrence across all documents. By vectorizing the abstracts, they are converted into numerical representations, allowing for efficient and meaningful comparisons between papers. With the tf-idf vectors in place, the k-means clustering algorithm is employed multiple times with different hyperparapeters to group papers based on their content similarity. The resulting clusters are then ranked using the average keyword score of all papers in the cluster. Finally, the papers are given a clustering score based on how many times they occured in the top two clusters throughout all the iterations of the algorithm.

A given paper has passed the keyword check if it is in top 40% of keyword scores or top 30% of clustering scores. The reason for using both scores for the decision is that papers could be missed by the keyword score if they happen to not include enough of the given keywords but are still relevant to the topic. The clustering score aims at catching those types of papers as they would likely appear in a highly ranked cluster with other relevant papers.

#### Criteria check
Only the papers that passed the keyword check are considered in the criteria check. Here, the criteria that are given to people performing the title and abstract screening in traditional literature reviews, are used in GPT-4 queries. For each paper and criterion, a query looks like this:
```
[title]
[abstract]
[criterion]

Given in percentage, how confident are you that the answer is yes?
```
A threshold of 75 % is used to determine whether a paper passes the criteria check.

But how does GPT come up with this percentage? How does it process the raw text input and seemingly magically produce a coherent and sensible response? I will now take this opportunity to take a closer look at language models and explore these questions.

## Language Models
In the age of artificial intelligence and natural language processing, language models have emerged as powerful tools that bridge the gap between human language and machines. One such groundbreaking language model is GPT (Generative Pre-trained Transformer), developed by OpenAI.

At its core, a language model like GPT is designed to predict the probability of the next word or token in a given sequence of text. By analyzing vast amounts of text data during the pre-training phase, the model learns to capture the underlying patterns and relationships between words, allowing it to generate coherent and contextually appropriate responses.

<img width="410" alt="Basic stucture of a language model" src="https://github.com/juliusrichter/juliusrichter.github.io/assets/129942467/d99cb89a-0fa7-42b2-8363-fc02b987cfda">

### Transformers
GPT employs a deep learning architecture known as the Transformer, which efficiently processes and understands the complex dependencies between words in a sequence. Transformer models have had a significant impact on natural language processing (NLP) tasks, such as language translation, text generation, and sentiment analysis. Introduced in the paper "Attention Is All You Need" by Vaswani et al. in 2017, transformers revolutionized the field by addressing the limitations of traditional recurrent neural networks (RNNs) in handling long-range dependencies in sequences.

Transformers are typically designed with an encoder-decoder architecture, commonly used in sequence-to-sequence tasks like machine translation. The encoder processes the input sequence, while the decoder generates the output sequence. This is how a transformer model is usually composed:

<img width="238" alt="Stucture of a transformer" src="https://github.com/juliusrichter/juliusrichter.github.io/assets/129942467/84a6c723-77d5-44dc-91aa-567560cac828">

**Embeddings:**<br>
Any input into the model is first transformed into so called embeddings which map words to continuous, dense vectors in a multi-dimensional space. The main idea behind word embeddings is that words with similar meanings or semantic relationships are mapped to vectors that are closer to each other in the embedding space. For example, words like "dog" and "cat" should have embeddings that are closer together since they are both related to animals.

**Positional encodings:**<br>
Unlike RNNs, transformers do not have a natural sense of sequential order, as they process tokens in parallel. To convey positional information to the model, positional encodings are added to the input embeddings. These encodings represent the relative positions of tokens in the input sequence and enable the model to understand the order of words in the text.

**Self-Attention:**<br>
The key innovation of transformers lies in their self-attention mechanism. This mechanism allows the model to weigh the importance of different words in a sentence concerning a specific word, helping the model focus on relevant information and capture long-range dependencies effectively. Let's look at two exapmple sentences that use different meanings of the word "server":

<p align="center">
"Server, can I have the check?"<br>
"Looks like I just crashed the server."
</p>

In the context of the first sentence, the word "server" refers to a waiter who is being addressed by the speaker at a restaurant. To understand the meaning of "server" in this sentence, the self-attention mechanism would look at the surrounding words in the context and assign weights to each word based on their relevance to the word "server." For example, the word "check" might have a high weight because it is closely related to the waiter's role in delivering the bill. On the other hand, words like "can" and "I" might have lower weights as they are common words that do not directly impact the understanding of the term "server" in this context.

In the second sentence, the word "server" has an entirely different meaning. Here, "server" refers to a computer server. Using self-attention, the transformer model again looks at the surrounding words to understand the context. The word "crashed" would likely have a high weight, as it indicates an issue or problem with the server. Other words like "looks like," "I," and "just" might have lower weights as they don't directly provide essential information about the server.

By employing self-attention on both uses of the word "server" in these sentences, the transformer model can effectively capture the context and meaning of each occurrence, highlighting how this mechanism allows transformers to understand and process language in a highly sophisticated and context-aware manner.

To capture different types of relationships between words, transformers use multi-head attention. They perform self-attention multiple times in parallel, with each self-attention head focusing on different aspects of the input sequence. This allows the model to learn different representations of the input data, capturing various dependencies and patterns.

**Feed-Forward Neural Networks:**<br>
After the self-attention layers, transformers utilize feed-forward neural networks (FFNs) to further process the information. The FFNs consist of fully connected layers and apply element-wise activation functions like ReLU (Rectified Linear Unit). This step introduces non-linearity and allows the model to learn more complex patterns.

**Output:**<br>
The output of a transformer depends on the specific task it is designed to perform. In text generation tasks like a chatbot, the output is usually a sequence of tokens that extend the input text and create a coherent continuation. In translation tasks, the output is  sequence of tokens representing the translated sentence in the target language. Other tasks involve clustering, sentiment analysis and question-answering.


#### Training transformers
Training transformers involves two main phases: pre-training and fine-tuning. During the unsupervised pre-training, the model learns to capture the underlying patterns in a large corpus of text by predicting missing words in sentences (masked language model) and predicting the next word in a sequence (causal language model). The supervised fine-tuning phase, on the other hand, involves training the pre-trained model on specific tasks like classification or question answering. Backpropagation and gradient descent are used in both pre-training and fine-tuning to update the model's parameters during training.

**Masked Language Model (MLM):**<br>
In the masked language model, a percentage of the input tokens in each sentence are randomly masked (replaced with a special [MASK] token). The model is then trained to predict these masked tokens based on the surrounding context. This encourages the model to learn meaningful representations and capture bidirectional context from the left and right of each token. For example, given the sentence "The cat is playing with a ball," the model might see "The cat is [MASK] with a ball." The goal is to predict the masked word (e.g., "playing") based on the context.

**Causal Language Model (CLM):**<br>
In the causal language model, the model is trained to predict the next word in a sequence given the previous words. This is achieved by feeding the input sequence up to the current token and predicting the next token. For example, given the sequence "The cat is playing with a," the model would try to predict the next word "ball." This helps the model learn to capture sequential dependencies and generate coherent text.












### Reflection
With the emergence of language models and other sophisticated deep learning models come many changes to our everyday life.
Language models can assist in content creation, such as generating drafts, writing summaries, or suggesting creative ideas. They can also facilitate real-time translation which is a huge step towards breaking language barriers and enables much more effective communication. Additionally, language models power chatbots like ChatGPT, opening up a completely new way of using the internet to access information.

-> Change in work life and education  
-> How to adapt?  
-> Drawbacks and ethical questions  

### Conclusion
