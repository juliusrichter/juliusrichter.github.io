# Automating Literature Reviews using Deep Learning Methods

### Introduction
Literature reviews play a crucial role in academic research, providing a comprehensive overview of existing knowledge on a particular topic. They are often times carried out at universities and pharmaceutical companiesand require an extensive amount of reading and analysis of scientific publications.

A typical way of conducting a manual literature review consists of several steps.
First, a huge database, such as PubMed, is queried for thousands of studies based on some keywords and basic criteria
that are determined beforehand and describe the given topic.
Next, all titles and abstracts of the returned studies are manually read and a small percentage (3-5 %) is selected
for further review. This is guided by more precise criteria that are too complicated for data base querying.
Finally, a full text review is performed on the remaining studies in order to make sure that only the relevant ones are
included.

However, given the time-consuming nature of reading thousands of abstracts and hundreds of full studies, there is growing interest in automating certain aspects of literature reviews.
Besides saving time, other benefits of automation include a potentially improved accuracy and objectivity, as human error can be minimized and a difference in approaches by different people is avoided. Additionally, automation tools can efficiently search across various databases and sources, ensuring a more comprehensive coverage of relevant studies.

In this blog, I will look at the tool Litverse and its approach to partially automating literature reviews with the help of GPT-4 and delve into the inner workings of language models like GPT.

### Litverse
Litverse is a tool that aims at automating literature reviews using several NLP and deep learning methods. Its basic concept is to take preselected papers and a number of criteria and keywords as input and to then output a score and ranking for each paper. The scoring process is split up into two steps - a keyword check and a criteria check.

#### Keyword check
To begin, more specific keywords are manually defined and assigned a priority score ranging from 1 to 3, with 1 being the lowest priority and 3 the highest. This approach ensures that essential concepts required in the literature review are given appropriate weightage.

The next step involves scoring the keywords within each paper. Occurrences of the keywords in the paper's title, abstract, and keyword list are counted and multiplied by their respective priority. This results in a keyword score for each paper that serves as a baseline score for the upcoming clustering step.

To facilitate paper clustering, tf-idf (Term Frequency-Inverse Document Frequency) vectorization is used on the abstracts of all papers. Tf-idf is a popular technique in natural language processing that evaluates the importance of a word in a document relative to its occurrence across all documents. By vectorizing the abstracts, they are converted into numerical representations, allowing for efficient and meaningful comparisons between papers. With the tf-idf vectors in place, the k-means clustering algorithm is employed multiple times with different hyperparapeters to group papers based on their content similarity. The resulting clusters are then ranked using the average keyword score of all papers in the cluster. Finally, the papers are given a clustering score based on how many times they occured in the top two clusters throughout all the iterations of the algorithm.

A given paper has passed the keyword check if it is in top 40% of keyword scores or top 30% of clustering scores. The reason for using both scores for the decision is that papers could be missed by the keyword score if they happen to not include enough of the given keywords but are still relevant to the topic. The clustering score aims at catching those types of papers as they would likely appear in a highly ranked cluster with other relevant papers.

#### Criteria check
Only the papers that passed the keyword check are considered in the criteria check. Here, the criteria that are given to people performing the title and abstract screening in traditional literature reviews, are used in GPT-4 queries. For each paper and criterion, a query looks like this:

_title_
_abstract_
_criterion_

_Given in percentage, how confident are you that the answer is yes?_

A threshold of 75 % is used to determine whether a paper passes the criteria check.

But how does GPT come up with this percentage? How does it process the raw text input and seemingly magically produce a coherent and sensible response? I will now take this opportunity to take a closer look at language models and explore these questions.

### Language Models
In the age of artificial intelligence and natural language processing, language models have emerged as powerful tools that bridge the gap between human language and machines. One such groundbreaking language model is GPT (Generative Pre-trained Transformer), developed by OpenAI.

At its core, a language model like GPT is designed to predict the probability of the next word or token in a given sequence of text. By analyzing vast amounts of text data during the pre-training phase, the model learns to capture the underlying patterns and relationships between words, allowing it to generate coherent and contextually appropriate responses.

<img width="410" alt="image" src="https://github.com/juliusrichter/juliusrichter.github.io/assets/129942467/d99cb89a-0fa7-42b2-8363-fc02b987cfda">

#### Pre-training Phase
The pre-training phase is a crucial step in the development of a language model like GPT. During this phase, the model is exposed to massive corpora of text, such as books, articles, and websites. Before the text is fed into the model, it undergoes a process called tokenization. Tokenization involves breaking down the text into smaller units called tokens, which can be individual words, subwords, or characters. This step ensures that the model can efficiently process the text and handle varying lengths of input.

#### Transformers
GPT employs a deep learning architecture known as the Transformer, which efficiently processes and understands the complex dependencies between words in a sequence. Transformer models have had a significant impact on natural language processing (NLP) tasks, such as language translation, text generation, and sentiment analysis. Introduced in the paper "Attention Is All You Need" by Vaswani et al. in 2017, transformers revolutionized the field by addressing the limitations of traditional recurrent neural networks (RNNs) in handling long-range dependencies in sequences.

The key innovation of transformers lies in their self-attention mechanism. This mechanism allows the model to weigh the importance of different words in a sentence concerning a specific word, helping the model focus on relevant information and capture long-range dependencies effectively. Let's look at two exapmple sentences that use different meanings of the word "server":

<p align="center">
”**Server**, can I have the check?”
“Looks like I just crashed the **server**.”
</p>

In the context of the first sentence, the word "server" refers to a waiter who is being addressed by the speaker at a restaurant. To understand the meaning of "server" in this sentence, the self-attention mechanism would look at the surrounding words in the context and assign weights to each word based on their relevance to the word "server." For example, the word "check" might have a high weight because it is closely related to the waiter's role in delivering the bill. On the other hand, words like "can" and "I" might have lower weights as they are common words that do not directly impact the understanding of the term "server" in this context.

In the second sentence, the word "server" has an entirely different meaning. Here, "server" refers to a computer server. Using self-attention, the transformer model again looks at the surrounding words to understand the context. The word "crashed" would likely have a high weight, as it indicates an issue or problem with the server. Other words like "looks like," "I," and "just" might have lower weights as they don't directly provide essential information about the server.

By employing self-attention on both uses of the word "server" in these sentences, the transformer model can effectively capture the context and meaning of each occurrence, highlighting how this mechanism allows transformers to understand and process language in a highly sophisticated and context-aware manner.









### Reflection
With the emergence of language models and other sophisticated deep learning models come many changes to our everyday life.
Language models can assist in content creation, such as generating drafts, writing summaries, or suggesting creative ideas. They can also facilitate real-time translation which is a huge step towards breaking language barriers and enables much more effective communication. Additionally, language models power chatbots like ChatGPT, opening up a completely new way of using the internet to access information.

-> Change in work life and education  
-> How to adapt?  
-> Drawbacks and ethical questions  

### Conclusion
